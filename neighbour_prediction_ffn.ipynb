{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "from typing import Dict, Tuple\n",
    "from typing import List, Set\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from graph import Graph\n",
    "from part import Part\n",
    "from preprocess import create_family_id_mapping\n",
    "from utils import load_graphs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "graphs = load_graphs()\n",
    "mapping = create_family_id_mapping(graphs)\n",
    "\n",
    "seed = 5\n",
    "random = random.Random(seed)\n",
    "random.shuffle(graphs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_features(graph: Graph, dense_family_id_mapping: Dict) -> Tuple[List[Tensor], List[Tensor]]:\n",
    "    num_different_family_ids = len(dense_family_id_mapping)\n",
    "\n",
    "    # Create a tensor for existing nodes and count their occurrences\n",
    "    all_nodes_tensor = torch.zeros(num_different_family_ids, dtype=torch.float)\n",
    "    nodes = graph.get_nodes()\n",
    "    edges = graph.get_edges()\n",
    "\n",
    "    feature_tensors = []\n",
    "    target_tensors = []\n",
    "\n",
    "    for node in nodes:\n",
    "        dense_family_id = dense_family_id_mapping[int(node.get_part().get_family_id())]\n",
    "        all_nodes_tensor[dense_family_id] += 1.0\n",
    "\n",
    "        target_tensor = torch.zeros(num_different_family_ids, dtype=torch.float)\n",
    "        for neighbour_node in edges[node]:\n",
    "            neighbour_node_dense_family_id = dense_family_id_mapping[int(neighbour_node.get_part().get_family_id())]\n",
    "            target_tensor[neighbour_node_dense_family_id] += 1\n",
    "        target_tensors.append(target_tensor)\n",
    "\n",
    "    for node in nodes:\n",
    "        dense_family_id = dense_family_id_mapping[int(node.get_part().get_family_id())]\n",
    "        given_node_tensor = torch.zeros(num_different_family_ids, dtype=torch.float)\n",
    "        given_node_tensor[dense_family_id] = 1\n",
    "\n",
    "        feature_tensors.append(torch.cat((all_nodes_tensor, given_node_tensor), dim=-1))\n",
    "\n",
    "    return feature_tensors, target_tensors\n",
    "    # return list(zip(feature_tensors, target_tensors))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split data into train, val and test - they are already shuffled\n",
    "\n",
    "train_upper = ceil(0.8 * len(graphs))\n",
    "val_upper = ceil(0.9 * len(graphs))\n",
    "\n",
    "train_graphs = graphs[0:train_upper]\n",
    "val_graphs = graphs[train_upper:val_upper]\n",
    "test_graphs = graphs[val_upper:len(graphs) + 1]\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graphs: List[Graph]):\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        for graph in graphs:\n",
    "            x_per_graph, y_per_graph = create_features(graph, mapping)\n",
    "            x_list += x_per_graph\n",
    "            y_list += y_per_graph\n",
    "        self.x_tensor = torch.stack(x_list)\n",
    "        self.y_tensor = torch.stack(y_list)\n",
    "        self.n_samples = self.y_tensor.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_tensor[index], self.y_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset=GraphDataset(train_graphs), batch_size=batch_size,\n",
    "                          shuffle=False)  #TODO: add num_workers=8\n",
    "val_loader = DataLoader(dataset=GraphDataset(val_graphs), batch_size=batch_size,\n",
    "                         shuffle=False)  #TODO: add num_workers=8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss 0.25635042786598206\n",
      "train_accuracy 0.4517391324043274\n",
      "loss 0.023113489151000977\n",
      "train_accuracy 0.9835869669914246\n",
      "loss 0.01993025839328766\n",
      "train_accuracy 0.9833695888519287\n",
      "loss 0.019973130896687508\n",
      "train_accuracy 0.9856522083282471\n",
      "loss 0.01798643358051777\n",
      "train_accuracy 0.9873913526535034\n",
      "loss 0.01799234189093113\n",
      "train_accuracy 0.9878261089324951\n",
      "loss 0.014070602133870125\n",
      "train_accuracy 0.9889130592346191\n",
      "validation acc: 0.9889240860939026\n",
      "epoch 1\n",
      "loss 0.01159029733389616\n",
      "train_accuracy 0.9908695816993713\n",
      "loss 0.01155709195882082\n",
      "train_accuracy 0.9900000095367432\n",
      "loss 0.011953765526413918\n",
      "train_accuracy 0.990760862827301\n",
      "loss 0.013188793323934078\n",
      "train_accuracy 0.990217387676239\n",
      "loss 0.011146407574415207\n",
      "train_accuracy 0.9908695816993713\n",
      "loss 0.01040367316454649\n",
      "train_accuracy 0.9908695816993713\n",
      "loss 0.00917848851531744\n",
      "train_accuracy 0.9919565320014954\n",
      "validation acc: 0.9916764497756958\n",
      "epoch 2\n",
      "loss 0.007789890747517347\n",
      "train_accuracy 0.9936956763267517\n",
      "loss 0.007262596394866705\n",
      "train_accuracy 0.9934782981872559\n",
      "loss 0.008562355302274227\n",
      "train_accuracy 0.9925000071525574\n",
      "loss 0.009075022302567959\n",
      "train_accuracy 0.9927173852920532\n",
      "loss 0.007828430272638798\n",
      "train_accuracy 0.9931522011756897\n",
      "loss 0.0070568714290857315\n",
      "train_accuracy 0.9933695793151855\n",
      "loss 0.006636039353907108\n",
      "train_accuracy 0.9934782981872559\n",
      "validation acc: 0.9935898780822754\n",
      "epoch 3\n",
      "loss 0.00565430847927928\n",
      "train_accuracy 0.9951087236404419\n",
      "loss 0.005175687838345766\n",
      "train_accuracy 0.9953261017799377\n",
      "loss 0.006221036426723003\n",
      "train_accuracy 0.994347870349884\n",
      "loss 0.006463917437940836\n",
      "train_accuracy 0.9944565296173096\n",
      "loss 0.006107062567025423\n",
      "train_accuracy 0.9944565296173096\n",
      "loss 0.005211788695305586\n",
      "train_accuracy 0.9951087236404419\n",
      "loss 0.00501968152821064\n",
      "train_accuracy 0.9952173829078674\n",
      "validation acc: 0.9949602484703064\n",
      "epoch 4\n",
      "loss 0.0045527801848948\n",
      "train_accuracy 0.9961956739425659\n",
      "loss 0.004055339377373457\n",
      "train_accuracy 0.9963043928146362\n",
      "loss 0.004937495104968548\n",
      "train_accuracy 0.9954348206520081\n",
      "loss 0.00517918961122632\n",
      "train_accuracy 0.9957609176635742\n",
      "loss 0.005080113653093576\n",
      "train_accuracy 0.9950000047683716\n",
      "loss 0.0044678328558802605\n",
      "train_accuracy 0.9957609176635742\n",
      "loss 0.004330082330852747\n",
      "train_accuracy 0.9955434799194336\n",
      "validation acc: 0.9957564473152161\n"
     ]
    }
   ],
   "source": [
    "input_size = 2 * len(mapping)\n",
    "hidden_size = 3 * len(mapping)\n",
    "output_size = len(mapping)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.fc2 = nn.Linear(..., ...)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()  #nn.BCELoss()\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch {epoch}\")\n",
    "    for i, (x_train, y_train) in enumerate(train_loader):\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        y_pred = model(x_train)\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        train_accuracy = torch.sum(y_train == y_pred.round()) / torch.numel(y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"loss {loss}\")\n",
    "            print(f\"train_accuracy {train_accuracy}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_accuracies = []\n",
    "        for i, (x_val, y_val) in enumerate(val_loader):\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            y_pred = model(x_val)\n",
    "            val_accuracy = torch.sum(y_val == y_pred.round()) / torch.numel(y_val)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "        print(f\"validation acc: {torch.mean(torch.stack(val_accuracies))}\")\n",
    "# plt.plot(train_loss)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9641430805678898"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import __generate_part_list_permutations\n",
    "import numpy as np\n",
    "\n",
    "def edge_accuracy(predicted_graph: Graph, target_graph: Graph) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of correct predicted edges.\n",
    "    :param predicted_graph:\n",
    "    :param target_graph:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(predicted_graph.get_nodes()) == len(target_graph.get_nodes()), 'Mismatch in number of nodes.'\n",
    "    assert predicted_graph.get_parts() == target_graph.get_parts(), 'Mismatch in expected and given parts.'\n",
    "\n",
    "    best_score = 0\n",
    "\n",
    "    # Determine all permutations for the predicted graph and choose the best one in evaluation\n",
    "    perms: List[Tuple[Part]] = __generate_part_list_permutations(predicted_graph.get_parts())\n",
    "    # Determine one part order for the target graph\n",
    "    target_parts_order = perms[0]\n",
    "    target_adj_matrix = target_graph.get_adjacency_matrix(target_parts_order)\n",
    "\n",
    "    if len(perms) > 80_000:\n",
    "        return 0\n",
    "    for perm in perms:\n",
    "        predicted_adj_matrix = predicted_graph.get_adjacency_matrix(perm)\n",
    "        score = np.sum(predicted_adj_matrix == target_adj_matrix)\n",
    "        best_score = max(best_score, score)\n",
    "\n",
    "    return best_score\n",
    "\n",
    "# from evaluation import edge_accuracy\n",
    "\n",
    "dense_family_id_to_family_id = {v: k for k, v in mapping.items()}\n",
    "edge_accuracies = []\n",
    "cpu_model = model.cpu()\n",
    "with torch.no_grad():\n",
    "    for index, test_graph in enumerate(test_graphs[:200]): #TODO remove :200\n",
    "        # matrix dimensions: x = nodes_list index, y = dense family id\n",
    "        predicted_adjacency_matrix = []\n",
    "        predicted_graph_count = 0\n",
    "        predicted_graph = Graph()\n",
    "        x, y = create_features(test_graph, mapping)\n",
    "        for x_test, y_test in zip(x, y):\n",
    "            y_test_pred = cpu_model(x_test)\n",
    "            predicted_adjacency_matrix.append(y_test_pred)\n",
    "        predicted_adjacency_matrix = torch.stack(predicted_adjacency_matrix)\n",
    "\n",
    "        nodes = test_graph.get_nodes()\n",
    "        nodes_list = list(nodes)\n",
    "        node_count = len(nodes)\n",
    "        added_nodes = set()\n",
    "        last_rep_node_a_index = -1\n",
    "        last_node_b_dense_family_id = -1\n",
    "        predicted_graph_edge_count = 0\n",
    "        while True:\n",
    "            max_pos_tensor = (predicted_adjacency_matrix == torch.max(predicted_adjacency_matrix)).nonzero()\n",
    "            rep_node_a_index = max_pos_tensor[0][0].item()\n",
    "            node_b_dense_family_id = max_pos_tensor[0][1].item()\n",
    "            if rep_node_a_index == last_rep_node_a_index and node_b_dense_family_id == last_node_b_dense_family_id:\n",
    "                break\n",
    "            last_rep_node_a_index = rep_node_a_index\n",
    "            last_node_b_dense_family_id = node_b_dense_family_id\n",
    "            node_b_family_id = dense_family_id_to_family_id[node_b_dense_family_id]\n",
    "            node_a_family_id = int(nodes_list[rep_node_a_index].get_part().get_family_id())\n",
    "            node_a_dense_family_id = mapping[node_a_family_id]\n",
    "\n",
    "            # node_a = nodes_list[node_a_index]\n",
    "            # node_a_candidates = list(filter(lambda node_a: int(node_a.get_part().get_family_id()) == node_a_family_id,nodes_list))\n",
    "            # node_b_candidates = list(filter(lambda node_b: int(node_b.get_part().get_family_id()) == node_b_family_id,nodes_list))\n",
    "            # if len(node_a_candidates) == len(node_b_candidates):\n",
    "            #     candidate_pairs = zip(node_a_candidates,node_b_candidates)\n",
    "            # else:\n",
    "            for node_a_index, node_a in enumerate(nodes_list):\n",
    "                if int(node_a.get_part().get_family_id()) == node_a_family_id:\n",
    "                    for node_b_index, node_b in enumerate(nodes_list):\n",
    "                        if int(node_b.get_part().get_family_id()) == node_b_family_id:\n",
    "                            if not (node_a in added_nodes and node_b in added_nodes):\n",
    "                                predicted_graph.add_undirected_edge(node_a.get_part(), node_b.get_part())\n",
    "                                predicted_graph_edge_count += 1\n",
    "                                added_nodes.add(node_a)\n",
    "                                added_nodes.add(node_b)\n",
    "\n",
    "                            # non-necessary performance optimization\n",
    "                            predicted_adjacency_matrix[node_a_index][node_b_dense_family_id] = 0.0\n",
    "                            predicted_adjacency_matrix[node_b_index][node_a_dense_family_id] = 0.0\n",
    "            predicted_adjacency_matrix[rep_node_a_index][node_b_dense_family_id] = 0.0\n",
    "            if predicted_graph_edge_count == node_count - 1:\n",
    "                break\n",
    "        edge_accuracies.append(edge_accuracy(test_graph, predicted_graph) / (node_count * node_count))\n",
    "sum(edge_accuracies) / len(edge_accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6707963520575578"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy of randomly generated trees\n",
    "edge_accuracies = []\n",
    "for test_graph in test_graphs[:100]:\n",
    "    predicted_graph = Graph()\n",
    "    nodes = test_graph.get_nodes()\n",
    "    node_count = len(nodes)\n",
    "    random_nodes_list = list(nodes)\n",
    "    random.shuffle(random_nodes_list)\n",
    "    node_a_index = 0\n",
    "    node_b_index = 1\n",
    "\n",
    "    for node_a_index in range(node_count - 1):\n",
    "        existing_node_index = random.randint(0, node_a_index)\n",
    "        existing_node = random_nodes_list[existing_node_index]\n",
    "        new_node_index = node_a_index + 1\n",
    "        new_node = random_nodes_list[new_node_index]\n",
    "        predicted_graph.add_undirected_edge(existing_node.get_part(), new_node.get_part())\n",
    "\n",
    "    edge_accuracies.append(edge_accuracy(test_graph, predicted_graph) / (node_count * node_count))\n",
    "\n",
    "sum(edge_accuracies) / len(edge_accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
