{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "from typing import Dict, Tuple\n",
    "from typing import List, Set\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from graph import Graph\n",
    "from part import Part\n",
    "from preprocess import create_family_id_mapping\n",
    "from utils import load_graphs\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "graphs = load_graphs()\n",
    "mapping = create_family_id_mapping(graphs)\n",
    "\n",
    "seed = 7\n",
    "random = random.Random(seed)\n",
    "random.shuffle(graphs)\n",
    "\n",
    "training_set_size_ratio = 0.8\n",
    "validation_set_size_ratio = 0.1\n",
    "batch_size = 1024\n",
    "\n",
    "input_size = 2 * len(mapping)  # 184\n",
    "output_size = len(mapping)  # 92\n",
    "\n",
    "input_size = input_size\n",
    "hidden_size = 2 * input_size\n",
    "output_size = output_size\n",
    "\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def create_features(graph: Graph, dense_family_id_mapping: Dict) -> Tuple[List[Tensor], List[Tensor]]:\n",
    "    num_different_family_ids = len(dense_family_id_mapping)\n",
    "\n",
    "    # Create a tensor for existing nodes and count their occurrences\n",
    "    all_nodes_tensor = torch.zeros(num_different_family_ids, dtype=torch.float)\n",
    "    nodes = graph.get_nodes()\n",
    "    edges = graph.get_edges()\n",
    "\n",
    "    feature_tensors = []\n",
    "    target_tensors = []\n",
    "\n",
    "    for node in nodes:\n",
    "        dense_family_id = dense_family_id_mapping[int(node.get_part().get_family_id())]\n",
    "        all_nodes_tensor[dense_family_id] += 1.0\n",
    "\n",
    "        target_tensor = torch.zeros(num_different_family_ids, dtype=torch.float)\n",
    "        for neighbour_node in edges[node]:\n",
    "            neighbour_node_dense_family_id = dense_family_id_mapping[int(neighbour_node.get_part().get_family_id())]\n",
    "            target_tensor[neighbour_node_dense_family_id] += 1\n",
    "        target_tensors.append(target_tensor)\n",
    "\n",
    "    for node in nodes:\n",
    "        dense_family_id = dense_family_id_mapping[int(node.get_part().get_family_id())]\n",
    "        given_node_tensor = torch.zeros(num_different_family_ids, dtype=torch.float)\n",
    "        given_node_tensor[dense_family_id] = 1\n",
    "\n",
    "        feature_tensors.append(torch.cat((all_nodes_tensor, given_node_tensor), dim=-1))\n",
    "\n",
    "    return feature_tensors, target_tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# split data into train, val and test - they are already shuffled\n",
    "\n",
    "train_upper = ceil(training_set_size_ratio * len(graphs))\n",
    "val_upper = ceil((training_set_size_ratio + validation_set_size_ratio) * len(graphs))\n",
    "\n",
    "train_graphs = graphs[0:train_upper]\n",
    "val_graphs = graphs[train_upper:val_upper]\n",
    "test_graphs = graphs[val_upper:len(graphs) + 1]\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graphs: List[Graph]):\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        for graph in graphs:\n",
    "            x_per_graph, y_per_graph = create_features(graph, mapping)\n",
    "            x_list += x_per_graph\n",
    "            y_list += y_per_graph\n",
    "        self.x_tensor = torch.stack(x_list)\n",
    "        self.y_tensor = torch.stack(y_list)\n",
    "        self.n_samples = self.y_tensor.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_tensor[index], self.y_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=GraphDataset(train_graphs),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=GraphDataset(val_graphs),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:31<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.fc2 = nn.Linear(..., ...)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")\n",
    "model = Net().to(device)\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs),ascii=\"░▒█\"):\n",
    "    train_losses_per_epoch = 0.0\n",
    "    train_accuracy_per_epoch = 0.0\n",
    "    for i, (x_train, y_train) in enumerate(train_loader):\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        y_pred = model(x_train)\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        train_losses_per_epoch += loss.cpu().detach().numpy()\n",
    "        train_accuracy = torch.sum(y_train == y_pred.round()) / torch.numel(y_train)\n",
    "        train_accuracy_per_epoch += train_accuracy.cpu().detach().numpy()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_losses.append(train_losses_per_epoch / len(train_loader))\n",
    "    train_accuracies.append(train_accuracy_per_epoch / len(train_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_losses_per_epoch = 0.0\n",
    "        val_accuracies_per_epoch = 0.0\n",
    "        for i, (x_val, y_val) in enumerate(val_loader):\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            y_pred = model(x_val)\n",
    "            loss = loss_fn(y_pred, y_val)\n",
    "            val_accuracy = torch.sum(y_val == y_pred.round()) / torch.numel(y_val)\n",
    "            val_losses_per_epoch += loss.cpu()\n",
    "            val_accuracies_per_epoch += val_accuracy.cpu()\n",
    "        val_losses.append(val_losses_per_epoch / len(val_loader))\n",
    "        val_accuracies.append(val_accuracies_per_epoch / len(val_loader))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "# plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1116 [00:02<01:40, 10.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[54], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m predicted_graph_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     12\u001B[0m predicted_graph \u001B[38;5;241m=\u001B[39m Graph()\n\u001B[1;32m---> 13\u001B[0m x, y \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_test, y_test \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(x, y):\n\u001B[0;32m     15\u001B[0m     y_test_pred \u001B[38;5;241m=\u001B[39m cpu_model(x_test)\n",
      "Cell \u001B[1;32mIn[34], line 5\u001B[0m, in \u001B[0;36mcreate_features\u001B[1;34m(graph, dense_family_id_mapping)\u001B[0m\n\u001B[0;32m      2\u001B[0m num_different_family_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(dense_family_id_mapping)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Create a tensor for existing nodes and count their occurrences\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m all_nodes_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_different_family_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m nodes \u001B[38;5;241m=\u001B[39m graph\u001B[38;5;241m.\u001B[39mget_nodes()\n\u001B[0;32m      7\u001B[0m edges \u001B[38;5;241m=\u001B[39m graph\u001B[38;5;241m.\u001B[39mget_edges()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from evaluation import edge_accuracy\n",
    "\n",
    "dense_family_id_to_family_id = {v: k for k, v in mapping.items()}\n",
    "edge_accuracies = []\n",
    "cpu_model = model.cpu()\n",
    "with torch.no_grad():\n",
    "    with tqdm(val_graphs) as val_graphs_with_progress:\n",
    "        for index, test_graph in enumerate(val_graphs_with_progress):\n",
    "            # matrix dimensions: x = nodes_list index, y = dense family id\n",
    "            predicted_adjacency_matrix = []\n",
    "            predicted_graph = Graph()\n",
    "            x, y = create_features(test_graph, mapping)\n",
    "            for x_test, y_test in zip(x, y):\n",
    "                y_test_pred = cpu_model(x_test)\n",
    "                predicted_adjacency_matrix.append(y_test_pred)\n",
    "            predicted_adjacency_matrix = torch.stack(predicted_adjacency_matrix)\n",
    "\n",
    "            nodes = test_graph.get_nodes()\n",
    "            nodes_list = list(nodes)\n",
    "            node_count = len(nodes)\n",
    "            added_nodes = set()\n",
    "            last_rep_node_a_index = -1\n",
    "            last_node_b_dense_family_id = -1\n",
    "            predicted_graph_edge_count = 0\n",
    "\n",
    "            while True:\n",
    "                max_pos_tensor = (predicted_adjacency_matrix == torch.max(predicted_adjacency_matrix)).nonzero()\n",
    "                rep_node_a_index = max_pos_tensor[0][0].item()\n",
    "                node_b_dense_family_id = max_pos_tensor[0][1].item()\n",
    "\n",
    "                if rep_node_a_index == last_rep_node_a_index and node_b_dense_family_id == last_node_b_dense_family_id:\n",
    "                    break\n",
    "                last_rep_node_a_index = rep_node_a_index\n",
    "                last_node_b_dense_family_id = node_b_dense_family_id\n",
    "\n",
    "                node_a_family_id = int(nodes_list[rep_node_a_index].get_part().get_family_id())\n",
    "                node_b_family_id = dense_family_id_to_family_id[node_b_dense_family_id]\n",
    "                node_a_dense_family_id = mapping[node_a_family_id]\n",
    "\n",
    "                for node_a_index, node_a in enumerate(nodes_list):\n",
    "                    if int(node_a.get_part().get_family_id()) == node_a_family_id:\n",
    "                        for node_b_index, node_b in enumerate(nodes_list):\n",
    "                            if int(node_b.get_part().get_family_id()) == node_b_family_id:\n",
    "                                if not (node_a in added_nodes and node_b in added_nodes):\n",
    "                                    predicted_graph.add_undirected_edge(node_a.get_part(), node_b.get_part())\n",
    "                                    predicted_graph_edge_count += 1\n",
    "                                    added_nodes.add(node_a)\n",
    "                                    added_nodes.add(node_b)\n",
    "\n",
    "                                # non-necessary performance optimization\n",
    "                                predicted_adjacency_matrix[node_a_index][node_b_dense_family_id] = 0.0\n",
    "                                predicted_adjacency_matrix[node_b_index][node_a_dense_family_id] = 0.0\n",
    "                predicted_adjacency_matrix[rep_node_a_index][node_b_dense_family_id] = 0.0\n",
    "                if predicted_graph_edge_count == node_count - 1:\n",
    "                    break\n",
    "            graph_edge_accuracy = edge_accuracy(test_graph, predicted_graph) / (node_count * node_count)\n",
    "            # print(f\"edge_accuracy: {graph_edge_accuracy}\")\n",
    "            # test_graph.draw()\n",
    "            # predicted_graph.draw()\n",
    "            # print(\"----\")\n",
    "\n",
    "            edge_accuracies.append(graph_edge_accuracy)\n",
    "sum(edge_accuracies) / len(edge_accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6657158222270694"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy of randomly generated trees\n",
    "edge_accuracies = []\n",
    "for test_graph in test_graphs[:100]:\n",
    "    predicted_graph = Graph()\n",
    "    nodes = test_graph.get_nodes()\n",
    "    node_count = len(nodes)\n",
    "    random_nodes_list = list(nodes)\n",
    "    random.shuffle(random_nodes_list)\n",
    "    node_a_index = 0\n",
    "    node_b_index = 1\n",
    "\n",
    "    for node_a_index in range(node_count - 1):\n",
    "        existing_node_index = random.randint(0, node_a_index)\n",
    "        existing_node = random_nodes_list[existing_node_index]\n",
    "        new_node_index = node_a_index + 1\n",
    "        new_node = random_nodes_list[new_node_index]\n",
    "        predicted_graph.add_undirected_edge(existing_node.get_part(), new_node.get_part())\n",
    "\n",
    "    edge_accuracies.append(edge_accuracy(test_graph, predicted_graph) / (node_count * node_count))\n",
    "\n",
    "sum(edge_accuracies) / len(edge_accuracies)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
